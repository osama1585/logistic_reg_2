{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee04547c-06a1-4692-94a1-167d6f42eb24",
   "metadata": {},
   "source": [
    "<span style=color:red;font-size:50px>ASSIGNMENT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f0dc4-857b-48b3-a0a0-fc07a17b6130",
   "metadata": {},
   "source": [
    "<span style=color:green;font-size:50px>LOGISTIC REGRESSION-2</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e9430-c93a-4b76-bcd4-ea4bf74427c1",
   "metadata": {},
   "source": [
    "# Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b17b09-a98e-46fc-8469-18a778009dfe",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f03787-e06a-4eb2-bfbe-fdfaefd50887",
   "metadata": {},
   "source": [
    "## Purpose and Working of Grid Search CV in Machine Learning\n",
    "\n",
    "### Purpose of Grid Search CV:\n",
    "Grid Search CV (Cross-Validation) is a technique used to find the optimal hyperparameters for a machine learning model. Hyperparameters are parameters that are not directly learned from the data but are set before the learning process begins. Examples include the regularization parameter in logistic regression, the depth of a decision tree, or the number of hidden layers in a neural network. The purpose of Grid Search CV is to systematically explore a range of hyperparameter values and find the combination that results in the best model performance.\n",
    "\n",
    "### How Grid Search CV Works:\n",
    "Grid Search CV works by exhaustively searching through a specified subset of hyperparameter combinations and evaluating each combination using cross-validation. Here's how it works:\n",
    "\n",
    "1. **Define Hyperparameter Grid**: Specify the hyperparameters and their respective ranges or values that you want to tune. For example, in a decision tree classifier, you might want to tune the maximum depth and minimum samples split.\n",
    "\n",
    "2. **Create Grid Search Object**: Instantiate a GridSearchCV object, providing it with the machine learning model (e.g., logistic regression, decision tree, etc.), the hyperparameter grid, and the cross-validation strategy (e.g., k-fold cross-validation).\n",
    "\n",
    "3. **Search for Best Parameters**: Fit the GridSearchCV object to the training data. Grid Search CV will then perform an exhaustive search over all hyperparameter combinations defined in the grid. For each combination, it trains the model using cross-validation on the training data and evaluates the model's performance using a specified evaluation metric (e.g., accuracy, F1 score, etc.).\n",
    "\n",
    "4. **Select Best Model**: After evaluating all combinations, Grid Search CV identifies the hyperparameter combination that resulted in the best performance based on the specified evaluation metric.\n",
    "\n",
    "5. **Final Model Training**: Optionally, you can retrain the model using the entire training dataset with the best hyperparameters found during grid search to obtain the final model.\n",
    "\n",
    "### Benefits of Grid Search CV:\n",
    "- **Systematic Exploration**: Grid Search CV systematically explores the hyperparameter space, ensuring that no potential combination is overlooked.\n",
    "- **Optimal Hyperparameters**: By evaluating each combination using cross-validation, Grid Search CV helps find hyperparameters that generalize well to unseen data.\n",
    "- **Improved Model Performance**: Finding the best hyperparameters often leads to improved model performance in terms of accuracy, precision, recall, or other evaluation metrics.\n",
    "- **Automation**: Grid Search CV automates the process of hyperparameter tuning, saving time and effort compared to manual tuning.\n",
    "\n",
    "In summary, Grid Search CV is a valuable tool in machine learning for finding the optimal hyperparameters for a model, ensuring better performance and generalization to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7d218-7294-4ce2-a060-b4e527227174",
   "metadata": {},
   "source": [
    "# Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9239ce-b419-4bc4-aa99-6c0f2ddaddf5",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf0cfc-7586-43e7-9a42-3a54afa295a2",
   "metadata": {},
   "source": [
    "## Difference Between Grid Search CV and Randomized Search CV\n",
    "\n",
    "### Grid Search CV:\n",
    "Grid Search CV (Cross-Validation) is an exhaustive search technique used to find the optimal hyperparameters for a machine learning model. It systematically searches through a specified subset of hyperparameter combinations and evaluates each combination using cross-validation. Grid Search CV evaluates all possible combinations of hyperparameters defined in a grid, making it computationally expensive, especially for large hyperparameter spaces.\n",
    "\n",
    "### Randomized Search CV:\n",
    "Randomized Search CV is an alternative hyperparameter tuning technique that randomly samples hyperparameter combinations from a specified distribution. Unlike Grid Search CV, which evaluates all possible combinations, Randomized Search CV evaluates a fixed number of random combinations. This approach is more efficient than Grid Search CV for large hyperparameter spaces because it does not need to evaluate every possible combination.\n",
    "\n",
    "### When to Choose Grid Search CV:\n",
    "- **Small Hyperparameter Space**: Grid Search CV is suitable for small hyperparameter spaces where the number of combinations is manageable.\n",
    "- **Exhaustive Search**: If computational resources permit and the hyperparameter space is not too large, Grid Search CV can provide a thorough exploration of the hyperparameter space.\n",
    "\n",
    "### When to Choose Randomized Search CV:\n",
    "- **Large Hyperparameter Space**: Randomized Search CV is preferable for large hyperparameter spaces where exhaustively searching all combinations is computationally expensive or impractical.\n",
    "- **Efficiency**: When computational resources are limited or when the hyperparameter search needs to be completed quickly, Randomized Search CV is more efficient than Grid Search CV.\n",
    "- **Exploration**: Randomized Search CV can be used to explore a wide range of hyperparameter values without exhaustively searching all combinations.\n",
    "\n",
    "### Considerations:\n",
    "- **Trade-off**: Grid Search CV provides a more exhaustive search but can be computationally expensive, while Randomized Search CV offers efficiency at the cost of potentially missing optimal hyperparameter combinations.\n",
    "- **Evaluation Metric**: Both techniques rely on cross-validation to evaluate hyperparameter combinations, so the choice between them may also depend on the evaluation metric used and the specific characteristics of the dataset.\n",
    "\n",
    "In summary, the choice between Grid Search CV and Randomized Search CV depends on factors such as the size of the hyperparameter space, computational resources, and the desired level of exploration of hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0585e5-a61d-4def-b291-11cfd97bb06c",
   "metadata": {},
   "source": [
    "# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d4a62-69ce-4d7a-9c17-8f20027d6b7d",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13173615-a0ab-47d3-ab7b-97e3491ba220",
   "metadata": {},
   "source": [
    "## Data Leakage in Machine Learning\n",
    "\n",
    "### What is Data Leakage?\n",
    "Data leakage, also known as information leakage or data snooping, occurs when information from outside the training dataset is used to create a machine learning model. This unintended access to information leads to the model learning patterns that are not generalizable to new, unseen data. Data leakage can occur at various stages of the machine learning pipeline, including during feature engineering, model training, and model evaluation.\n",
    "\n",
    "### Why is Data Leakage a Problem?\n",
    "Data leakage can severely impact the performance and reliability of machine learning models in several ways:\n",
    "1. **Overestimation of Performance**: Models trained on leaked data may achieve artificially high performance metrics during training and evaluation, leading to inflated expectations of model performance on unseen data.\n",
    "2. **Lack of Generalization**: Models may learn spurious correlations or patterns that do not generalize to new data, resulting in poor performance when deployed in real-world scenarios.\n",
    "3. **Ethical and Legal Concerns**: Leakage of sensitive or private information can lead to ethical and legal consequences, including breaches of privacy regulations and loss of trust in the model and organization.\n",
    "\n",
    "### Example of Data Leakage:\n",
    "Let's consider an example of data leakage in credit card fraud detection:\n",
    "- **Scenario**: Suppose you are building a machine learning model to detect fraudulent transactions using historical credit card transaction data.\n",
    "- **Data Leakage**: Inadvertently, you include the transaction timestamp as a feature in the model. During feature engineering, you calculate the time difference between each transaction and the previous transaction in the dataset.\n",
    "- **Problem**: The model may learn to exploit temporal patterns in the transaction timestamps, such as time of day or day of the week, to predict fraud. However, these patterns are not indicative of fraudulent behavior and may lead to false positives.\n",
    "- **Impact**: The model's performance metrics may appear high during training and evaluation due to the presence of these leaked features. However, when deployed in a real-world setting, the model fails to generalize, resulting in poor fraud detection performance and potentially significant financial losses.\n",
    "\n",
    "### Preventing Data Leakage:\n",
    "To prevent data leakage and ensure the integrity and reliability of machine learning models, it is essential to:\n",
    "- **Understand the Data**: Have a thorough understanding of the dataset and identify potential sources of leakage.\n",
    "- **Separate Training and Validation Data**: Ensure that information from the validation or test dataset does not leak into the training dataset.\n",
    "- **Feature Engineering**: Be cautious when creating features and avoid using information that would not be available at prediction time.\n",
    "- **Cross-Validation**: Use cross-validation techniques to evaluate model performance on multiple folds of the training data, preventing overfitting to specific subsets of the data.\n",
    "\n",
    "In summary, data leakage is a critical issue in machine learning that can lead to overestimation of model performance, lack of generalization, and ethical concerns. It is essential to be vigilant and proactive in identifying and preventing data leakage throughout the machine learning pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2806a1-3ca5-4660-bdd6-def82120b3fa",
   "metadata": {},
   "source": [
    "# Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22e302-5d52-46a5-a2ea-18ceac690f2e",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402403f-9197-41ba-a024-683a3b2fc8ae",
   "metadata": {},
   "source": [
    "## Preventing Data Leakage in Machine Learning\n",
    "\n",
    "Data leakage, also known as information leakage, occurs when information from outside the training dataset is used to create a machine learning model, leading to inflated performance metrics and unreliable model predictions. Preventing data leakage is crucial for building robust and trustworthy machine learning models. Here are several strategies to prevent data leakage:\n",
    "\n",
    "### 1. Data Preparation:\n",
    "- **Feature Selection**: Carefully select features that are relevant and available at prediction time. Avoid using features that leak information from the target variable or are not causally related to the outcome.\n",
    "- **Temporal Split**: If the data has a temporal component (e.g., time series data), split the dataset into training and validation sets based on time. Ensure that information from future time periods does not leak into the training data.\n",
    "\n",
    "### 2. Feature Engineering:\n",
    "- **Create Features Carefully**: Be cautious when creating features to ensure that they do not inadvertently leak information from the target variable. Avoid using features derived from the target variable or derived using future information.\n",
    "- **Use Encodings**: When encoding categorical variables, use techniques such as target encoding or leave-one-out encoding that do not leak information from the target variable.\n",
    "\n",
    "### 3. Cross-Validation:\n",
    "- **Use Proper Cross-Validation Techniques**: Use techniques like k-fold cross-validation to evaluate model performance on multiple subsets of the training data. Ensure that the validation data is strictly separate from the training data to prevent leakage.\n",
    "- **Time Series Cross-Validation**: If working with time series data, use time series cross-validation techniques such as forward chaining or rolling window validation.\n",
    "\n",
    "### 4. Model Training and Evaluation:\n",
    "- **Pipeline Design**: Use scikit-learn pipelines to encapsulate preprocessing steps, feature engineering, and model training. This ensures that preprocessing steps are applied consistently during training and evaluation.\n",
    "- **Evaluate on Holdout Data**: Reserve a separate holdout dataset that is not used for model training or hyperparameter tuning. Evaluate the final model on this holdout dataset to assess its performance on unseen data.\n",
    "\n",
    "### 5. Monitoring and Debugging:\n",
    "- **Monitor Model Performance**: Continuously monitor the model's performance on new data to detect any unexpected changes or degradation in performance, which could indicate data leakage or model drift.\n",
    "- **Debugging**: If unexpected results are observed, thoroughly investigate the data and model to identify potential sources of data leakage or other issues.\n",
    "\n",
    "### 6. Documentation and Communication:\n",
    "- **Document Data Sources**: Maintain detailed documentation of the data sources, preprocessing steps, and feature engineering techniques used in model development.\n",
    "- **Communication**: Foster communication and collaboration between data scientists, domain experts, and stakeholders to ensure a shared understanding of potential sources of data leakage and strategies for prevention.\n",
    "\n",
    "By following these preventive measures, data scientists can minimize the risk of data leakage and build more reliable and trustworthy machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407e12d-4030-4b98-a0c6-a30d82b571d8",
   "metadata": {},
   "source": [
    "# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806977c8-b09d-464d-a1c0-eb7aa166c739",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718eea4d-9af9-44b0-9854-eca0b9517011",
   "metadata": {},
   "source": [
    "## Confusion Matrix in Classification\n",
    "\n",
    "### What is a Confusion Matrix?\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification model. It presents a summary of the predictions made by the model on a test dataset compared to the actual ground truth labels. The matrix is organized into rows and columns, where each row represents the actual class, and each column represents the predicted class.\n",
    "\n",
    "### Components of a Confusion Matrix:\n",
    "A confusion matrix typically consists of four components:\n",
    "\n",
    "1. **True Positives (TP)**: The number of observations that were correctly predicted as positive (e.g., correctly identified as belonging to the positive class).\n",
    "2. **False Positives (FP)**: The number of observations that were incorrectly predicted as positive (e.g., incorrectly identified as belonging to the positive class when they actually belong to the negative class).\n",
    "3. **True Negatives (TN)**: The number of observations that were correctly predicted as negative (e.g., correctly identified as belonging to the negative class).\n",
    "4. **False Negatives (FN)**: The number of observations that were incorrectly predicted as negative (e.g., incorrectly identified as belonging to the negative class when they actually belong to the positive class).\n",
    "\n",
    "### Interpretation of a Confusion Matrix:\n",
    "A confusion matrix provides insights into the performance of a classification model by summarizing its predictions. Here's what each component tells us about the model's performance:\n",
    "\n",
    "- **True Positives (TP)**: The model correctly identified these instances as positive. A high TP indicates a good ability to detect positive cases.\n",
    "- **False Positives (FP)**: The model incorrectly classified these instances as positive. A high FP rate indicates a tendency to falsely label negative instances as positive.\n",
    "- **True Negatives (TN)**: The model correctly identified these instances as negative. A high TN indicates a good ability to identify negative cases.\n",
    "- **False Negatives (FN)**: The model incorrectly classified these instances as negative. A high FN rate indicates a tendency to miss positive instances.\n",
    "\n",
    "### Metrics Derived from Confusion Matrix:\n",
    "Several performance metrics can be derived from a confusion matrix, including:\n",
    "- **Accuracy**: The proportion of correctly classified instances (TP + TN) over the total number of instances.\n",
    "- **Precision**: The proportion of true positive predictions among all positive predictions (TP / (TP + FP)).\n",
    "- **Recall (Sensitivity)**: The proportion of true positive predictions among all actual positive instances (TP / (TP + FN)).\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "- **Specificity**: The proportion of true negative predictions among all actual negative instances (TN / (TN + FP)).\n",
    "- **False Positive Rate (FPR)**: The proportion of false positive predictions among all actual negative instances (FP / (FP + TN)).\n",
    "\n",
    "### Importance of Confusion Matrix:\n",
    "- **Performance Evaluation**: Confusion matrices provide a detailed breakdown of a model's predictions, allowing for a comprehensive evaluation of its performance.\n",
    "- **Error Analysis**: By examining the distribution of true positives, false positives, true negatives, and false negatives, data scientists can identify patterns and areas for improvement in the model.\n",
    "- **Model Comparison**: Confusion matrices facilitate the comparison of multiple models by comparing their performance metrics derived from the matrix.\n",
    "\n",
    "In summary, a confusion matrix is a valuable tool for evaluating the performance of classification models, providing insights into their ability to correctly classify instances into different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334138e4-6626-4016-9059-3aec5f2f08c4",
   "metadata": {},
   "source": [
    "# Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970c437-8de4-46fa-b3ed-2b1f37fc6495",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712aca0-4b29-4b1c-8958-16bee185da57",
   "metadata": {},
   "source": [
    "## Precision vs. Recall in Confusion Matrix\n",
    "\n",
    "### Precision:\n",
    "Precision, also known as positive predictive value, is a performance metric that measures the proportion of true positive predictions among all positive predictions made by a classification model. It focuses on the accuracy of positive predictions and answers the question: \"Of all instances predicted as positive, how many were actually positive?\"\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Where:\n",
    "- TP (True Positives) is the number of instances correctly predicted as positive.\n",
    "- FP (False Positives) is the number of instances incorrectly predicted as positive.\n",
    "\n",
    "### Recall:\n",
    "Recall, also known as sensitivity or true positive rate, is a performance metric that measures the proportion of true positive predictions among all actual positive instances in the dataset. It focuses on the model's ability to capture all positive instances and answers the question: \"Of all actual positive instances, how many were correctly predicted as positive?\"\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "Where:\n",
    "- TP (True Positives) is the number of instances correctly predicted as positive.\n",
    "- FN (False Negatives) is the number of instances incorrectly predicted as negative.\n",
    "\n",
    "### Difference between Precision and Recall:\n",
    "- **Precision**: Precision emphasizes the quality of positive predictions, indicating how precise the model is when it predicts a positive outcome. It is concerned with minimizing false positives.\n",
    "- **Recall**: Recall emphasizes the quantity of positive predictions, indicating how well the model captures all positive instances in the dataset. It is concerned with minimizing false negatives.\n",
    "\n",
    "### Trade-off between Precision and Recall:\n",
    "- Increasing precision typically leads to a decrease in recall and vice versa. This trade-off is inherent in many classification problems, and finding the right balance depends on the specific context and requirements of the task.\n",
    "- For example, in a medical diagnosis scenario, high precision is essential to avoid unnecessary treatments (minimizing false positives), while high recall is crucial to ensure all patients with the disease are correctly identified (minimizing false negatives).\n",
    "\n",
    "### Importance in Model Evaluation:\n",
    "- Precision and recall provide complementary insights into a classification model's performance, highlighting different aspects of its predictive ability.\n",
    "- Both metrics should be considered together when evaluating model performance, especially in scenarios where the costs of false positives and false negatives vary.\n",
    "\n",
    "In summary, precision and recall are key performance metrics derived from a confusion matrix that assess a classification model's ability to make accurate positive predictions and capture all positive instances, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bbec1-57fd-46b8-a12f-226747673b23",
   "metadata": {},
   "source": [
    "# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc8258-7569-4bc4-8925-6fa37640c8c7",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a91c52-0804-4bbb-9860-299423298ef9",
   "metadata": {},
   "source": [
    "## Interpreting a Confusion Matrix for Error Analysis\n",
    "\n",
    "A confusion matrix provides a detailed breakdown of a classification model's predictions compared to the actual ground truth labels. By analyzing the components of the confusion matrix, you can identify the types of errors your model is making and gain insights into its performance. Here's how you can interpret a confusion matrix:\n",
    "\n",
    "### 1. True Positives (TP):\n",
    "True positives represent instances that were correctly predicted as positive by the model. These are instances where the model correctly identified the positive class.\n",
    "\n",
    "### 2. False Positives (FP):\n",
    "False positives represent instances that were incorrectly predicted as positive by the model when they were actually negative. These are instances where the model falsely identified the positive class.\n",
    "\n",
    "### 3. True Negatives (TN):\n",
    "True negatives represent instances that were correctly predicted as negative by the model. These are instances where the model correctly identified the negative class.\n",
    "\n",
    "### 4. False Negatives (FN):\n",
    "False negatives represent instances that were incorrectly predicted as negative by the model when they were actually positive. These are instances where the model failed to identify the positive class.\n",
    "\n",
    "### Analyzing Errors:\n",
    "- **Type I Error (False Positive)**: When the model predicts a positive outcome when it should have predicted a negative outcome. This error is indicated by a high number of false positives (FP).\n",
    "- **Type II Error (False Negative)**: When the model predicts a negative outcome when it should have predicted a positive outcome. This error is indicated by a high number of false negatives (FN).\n",
    "- **Balancing Errors**: Analyze the trade-off between false positives and false negatives based on the specific requirements of the problem. For example, in medical diagnosis, false negatives (missing a positive case) may be more detrimental than false positives (incorrectly diagnosing a negative case).\n",
    "\n",
    "### Example Interpretation:\n",
    "- **High False Positive Rate**: If the model has a high number of false positives (FP), it may be overly sensitive and prone to making Type I errors.\n",
    "- **High False Negative Rate**: If the model has a high number of false negatives (FN), it may be conservative and prone to making Type II errors.\n",
    "- **Imbalanced Classes**: In scenarios with imbalanced classes, the confusion matrix helps identify which class is being misclassified more frequently, allowing for targeted improvements.\n",
    "\n",
    "### Iterative Improvement:\n",
    "- Use insights from the confusion matrix to iteratively improve the model by adjusting hyperparameters, feature engineering, or employing different modeling techniques.\n",
    "- Continuously monitor the confusion matrix as you refine the model to ensure that improvements are being made in the desired direction.\n",
    "\n",
    "By carefully analyzing the components of the confusion matrix, you can gain valuable insights into the types of errors your model is making and make informed decisions to improve its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea5b62-c48a-45b7-adb8-82c2bf07e1f8",
   "metadata": {},
   "source": [
    "# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579949a-a537-4d59-87d4-841f9a1a7db1",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cafc1-9415-488c-ac45-30feb8729d32",
   "metadata": {},
   "source": [
    "## Common Metrics Derived from a Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a detailed breakdown of a classification model's predictions compared to the actual ground truth labels. From a confusion matrix, several common performance metrics can be derived to assess the model's accuracy, precision, recall, and other aspects of its performance. Here are some common metrics and how they are calculated:\n",
    "\n",
    "### 1. Accuracy:\n",
    "Accuracy measures the proportion of correctly classified instances out of the total number of instances in the dataset.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "### 2. Precision:\n",
    "Precision measures the proportion of true positive predictions among all positive predictions made by the model.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "### 3. Recall (Sensitivity):\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions among all actual positive instances in the dataset.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "### 4. Specificity:\n",
    "Specificity measures the proportion of true negative predictions among all actual negative instances in the dataset.\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "### 5. False Positive Rate (FPR):\n",
    "False Positive Rate measures the proportion of false positive predictions among all actual negative instances in the dataset.\n",
    "\n",
    "FPR = FP / (FP + TN)\n",
    "\n",
    "### 6. F1 Score:\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "### 7. True Negative Rate (TNR):\n",
    "True Negative Rate measures the proportion of true negative predictions among all actual negative instances in the dataset. It is also known as specificity.\n",
    "\n",
    "TNR = TN / (TN + FP)\n",
    "\n",
    "### 8. False Discovery Rate (FDR):\n",
    "False Discovery Rate measures the proportion of false positive predictions among all positive predictions made by the model.\n",
    "\n",
    "FDR = FP / (FP + TP)\n",
    "\n",
    "### 9. False Omission Rate (FOR):\n",
    "False Omission Rate measures the proportion of false negative predictions among all negative predictions made by the model.\n",
    "\n",
    "FOR = FN / (FN + TN)\n",
    "\n",
    "### 10. Matthews Correlation Coefficient (MCC):\n",
    "The Matthews Correlation Coefficient is a measure of the quality of binary (two-class) classifications, taking into account true and false positives and negatives.\n",
    "\n",
    "MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "### Importance of Metrics:\n",
    "- Each metric provides a different perspective on the model's performance and behavior.\n",
    "- The choice of metrics depends on the specific requirements and goals of the classification task.\n",
    "- It is essential to consider multiple metrics together to obtain a comprehensive evaluation of the model's performance.\n",
    "\n",
    "In summary, various performance metrics can be derived from a confusion matrix to assess the accuracy, precision, recall, specificity, and other aspects of a classification model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45e42f-1898-460f-890e-6b7d7eefe83a",
   "metadata": {},
   "source": [
    "# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3477bf-aa51-4c18-9adb-5b1002a1efec",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2ac1f-9c17-4f10-b515-35379eb8a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
